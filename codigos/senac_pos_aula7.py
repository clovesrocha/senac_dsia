# -*- coding: utf-8 -*-
"""senac_pos_aula7.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19eSqy9C-sfI-aymXTj2OgRlH5jpufRe7

# Nova seção

#
1. Precisão, Revocação e F1-Score
Essas métricas são comumente usadas para avaliar modelos de classificação, incluindo tarefas de PLN como classificação de sentimentos ou identificação de entidades nomeadas.
Conceitos:
* Precisão (Precision): Das instâncias classificadas como positivas, qual proporção foi realmente positiva? Precisa˜o=Verdadeiros Positivos (VP)+Falsos Positivos (FP)Verdadeiros Positivos (VP)
* Revocação (Recall): De todas as instâncias realmente positivas, qual proporção foi corretamente identificada pelo modelo? Revocac¸ a˜o=Verdadeiros Positivos (VP)+Falsos Negativos (FN)Verdadeiros Positivos (VP)
* F1-Score: Uma média harmônica entre precisão e revocação. É útil quando você busca um equilíbrio entre as duas métricas. F1-Score=2×Precisa˜o+Revocac¸ a˜oPrecisa˜o×Revocac¸ a˜o
Exemplo de Código em Python (usando scikit-learn):
"""

from sklearn.metrics import precision_score, recall_score, f1_score

# Suponha que estas sejam as previsões do seu modelo e os rótulos verdadeiros
y_true = [0, 1, 0, 1, 1, 0, 1, 0, 0, 1]  # Rótulos verdadeiros (0 = negativo, 1 = positivo)
y_pred = [0, 1, 1, 1, 0, 0, 1, 1, 0, 1]  # Previsões do modelo

# Calculando a precisão
precision = precision_score(y_true, y_pred)
print(f"Precisão: {precision:.4f}")

# Calculando a revocação
recall = recall_score(y_true, y_pred)
print(f"Revocação: {recall:.4f}")

# Calculando o F1-Score
f1 = f1_score(y_true, y_pred)
print(f"F1-Score: {f1:.4f}")

"""#
Interpretação:
Neste exemplo, estamos comparando os rótulos verdadeiros (y_true) com as previsões do nosso modelo (y_pred). As funções precision_score, recall_score e f1_score do scikit-learn calculam as respectivas métricas.
2. Perplexidade (Perplexity)
A perplexidade é uma métrica comumente usada para avaliar modelos de linguagem. Ela mede o quão bem um modelo de linguagem prediz uma amostra de texto. Uma perplexidade menor indica que o modelo é mais confiante e preditivo.
Conceito:
Seja W=w1 ,w2 ,...,wN uma sequência de palavras, e P(wi ∣w1 ,...,wi−1 ) a probabilidade prevista pelo modelo para a i-ésima palavra dado o histórico anterior, a perplexidade do modelo no texto W é definida como:
Perplexidade(W)=(i=1∏N P(wi ∣w1 ,...,wi−1 )1 )1/N=exp(−N1 i=1∑N logP(wi ∣w1 ,...,wi−1 ))
Em termos mais simples, a perplexidade pode ser interpretada como o número médio de escolhas igualmente prováveis que o modelo tem ao prever a próxima palavra.
Exemplo de Código em Python (usando NLTK e um modelo de linguagem n-grama):

"""

from nltk.lm.models import MLE
from nltk.lm.preprocessing import padded_everygram_pipeline
from nltk.tokenize import word_tokenize
import numpy as np

# Texto de exemplo para treinar o modelo
texto = "o gato está no tapete o cachorro está no chão"
tokens = word_tokenize(texto.lower())

# Criando n-gramas (vamos usar bigramas, n=2)
n = 2
train_data, padded_sents = padded_everygram_pipeline(n, [tokens])

# Treinando o modelo MLE (Maximum Likelihood Estimation)
model = MLE(n)
model.fit(train_data, padded_sents)

# Novo texto para avaliar a perplexidade
texto_avaliacao = "o cão está no tapete"
tokens_avaliacao = word_tokenize(texto_avaliacao.lower())

# Calculando a perplexidade da frase
perplexidade = model.perplexity(tokens_avaliacao)
print(f"Perplexidade da frase '{texto_avaliacao}': {perplexidade:.4f}")

# Você também pode calcular a perplexidade em um corpus maior
texto_corpus = ["o rato roeu a roupa do rei de roma", "a aranha arranha a jarra"]
corpus_tokens = [word_tokenize(sent.lower()) for sent in texto_corpus]
_, corpus_padded_sents = padded_everygram_pipeline(n, corpus_tokens)

total_log_prob = 0
total_words = 0
for sentence in corpus_padded_sents:
    log_prob = model.log_perplexity(sentence)
    total_log_prob += log_prob * len(sentence)
    total_words += len(sentence)

corpus_perplexity = np.exp(-total_log_prob / total_words)
print(f"Perplexidade do corpus: {corpus_perplexity:.4f}")

"""# Observações:

Este exemplo usa um modelo de linguagem n-grama simples (MLE) do NLTK. Modelos de linguagem mais avançados, como os baseados em redes neurais (e.g., RNNs, Transformers), geralmente oferecem melhor desempenho e são avaliados da mesma forma usando a perplexidade.
A perplexidade é sensível ao vocabulário do modelo. Um modelo com um vocabulário limitado pode ter uma perplexidade artificialmente baixa em textos que usam apenas essas palavras.
Ao avaliar modelos de linguagem maiores e mais complexos (como GPT ou BERT para tarefas generativas), a perplexidade ainda é uma métrica útil, embora outras métricas específicas para a tarefa (como BLEU para tradução automática ou ROUGE para sumarização) também sejam importantes.
"""